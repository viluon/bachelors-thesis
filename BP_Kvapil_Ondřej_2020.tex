% vim: set tabstop=4 shiftwidth=4

% Neovim setup:
% let g:vimtex_compiler_latexrun = {
%	\ 'build_dir' : '',
%	\ 'options' : [
%	\   '-verbose-cmds',
%	\   '-xelatex',
%	\   '-shell-escape',
%	\   '-interaction=nonstopmode',
%	\   '-synctex=1',
%	\   '-file-line-error',
%	\   '--latex-args="-shell-escape"',
%	\ ],
%	\}

% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=B,english]{FITthesis}[2019/12/23]

\usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8
% \usepackage[latin2]{inputenc} % LaTeX source encoded as ISO-8859-2
% \usepackage[cp1250]{inputenc} % LaTeX source encoded as Windows-1250

% \usepackage{subfig} %subfigures
% \usepackage{amsmath} %advanced maths
% \usepackage{amssymb} %additional math symbols

\usepackage{dirtree} %directory tree visualisation
\usepackage{xcolor}
\usepackage{blindtext}
\usepackage{footnote}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{booktabs}
\usepackage[numbers]{natbib}
\usepackage[htt]{hyphenat}
\usepackage{xparse,minted}

% custom commands
\newcommand{\todo}[1]{\textcolor{red}{\textbf{[[#1]]}}}
\newcommand{\blind}[1][1]{\textcolor{gray}{\Blindtext[#1][1]}}
\newcommand{\citationNeeded}{\textcolor{red}{\textbf{[citation needed]}}}
\newcommand{\hackage}[1]{\texttt{#1}}
\newcommand{\hsSignature}[1]{\texttt{#1}}
\newcommand{\hsType}[1]{\texttt{#1}}
\newcommand{\hsIdent}[1]{\texttt{#1}}
\newcommand{\hsModule}[1]{\texttt{#1}}
\newcommand{\hsTC}[1]{\texttt{#1}}
\newcommand{\hsCode}[1]{\mintinline[breakbytokenanywhere,breaklines]{haskell}{#1}}

% tabularx customisation
\newcolumntype{L}{>{\RaggedRight\arraybackslash}X}


% list of acronyms
\usepackage[acronym,nonumberlist,toc,numberedsection=autolabel]{glossaries}
\iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
\makeglossaries

% \newacronym{CVUT}{{\v C}VUT}{{\v C}esk{\' e} vysok{\' e} u{\v c}en{\' i} technick{\' e} v Praze}
% \newacronym{FIT}{FIT}{Fakulta informa{\v c}n{\' i}ch technologi{\' i}}
\newacronym{ghc}{GHC}{Glasgow Haskell Compiler}
% TODO: isn't there a better glossary package that handles references
% correctly?
\newacronym{ghci}{GHCi}{\acrshort{ghc} interpreter}
\newacronym{rts}{RTS}{\acrshort{ghc} RunTime System}
\newacronym{th}{TH}{Template Haskell}
\newacronym{syb}{SYB}{Scrap Your Boilerplate}
\newacronym{repl}{REPL}{Read-Eval-Print Loop}
\newacronym{ast}{AST}{Abstract Syntax Tree}
\newacronym{csv}{CSV}{Comma-Separated Values}
\newacronym{api}{API}{Application Programming Interface}
\newacronym{rhs}{RHS}{Right-Hand Side}
\newacronym{hpc}{HPC}{Haskell Program Coverage}
% TODO this should probs go into a glossary instead?
\newacronym{stg}{STG}{Spineless Tag\-less G-machine, an abstract machine based
on graph reduction\cite{stg-classic}. \acrshort{ghc} compiles the Core language
to STG instructions, machine code is generated from the STG representation.}
% TODO this as well (the acronyms should only list the unabbreviated form, not
% a description of what these are)
\newacronym{gnu}{GNU}{GNU's Not Unix, a Unix-like operating system}
\newacronym{hls}{HLS}{Haskell Language Server}
\newacronym{bco}{BCO}{Byte Code Object}
% TODO citation?
\newacronym{lsp}{LSP}{Language Server Protocol}
\newacronym{tso}{TSO}{Thread State Object}
\newacronym{whnf}{WHNF}{Weak Head Normal Form}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% EDIT THIS
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\department{Programming Research Laboratory}
\title{Haskell Dynamic Tracing}
\authorGN{Ondřej} %author's given name/names
\authorFN{Kvapil} %author's surname
\author{Ondřej Kvapil} %author's name without academic degrees
\authorWithDegrees{Ondřej Kvapil} %author's name with academic degrees
\supervisor{Ing. Filip Křikava, Ph.D.}
\acknowledgements{THANKS (remove entirely in case you do not wish to thank anyone)}
\abstractEN{Lazy evaluation is a potentially powerful implementation strategy
	for non-strict languages, freeing the programmer to focus on what a program
	means rather than on how it is computed. Laziness naturally accommodates
	user-defined control flow and evaluates only the required subset of a given
	program in a demand-driven manner. However, delayed evaluation makes
	complexity analysis challenging and can lead to hard-to-predict memory
	behaviour. To better understand the trade-offs laziness offers and how it
	is used in practical scenarios, we design a dynamic tracing plugin for
	the \acrlong{ghc}, implement a proof of concept, and demonstrate its
	ability to record crucial information about the use of laziness in simple
	Haskell programs.}
\abstractCS{Líné vyhodnocování je potenciálně mocná implementační strategie pro
	non-strict programovací jazyky, která umožňuje programátorům soustředit se
	na to, co program znamená, aniž by byli rušeni způsobem jeho vyhodnocení.
	Lenost přináší možnost přirozeně vyjádřit uživatelem definované řídící
	konstrukce a vede k vyhodnocení jen potřebné části programu. Odklad
	vyhodnocování ale komplikuje analýzu složitosti a může vést k těžko
	předvídatelnému paměťovému chování. Pro lepší pochopení kompromisů
	spojených s leností a jejího využití v praktických situacích jsme navrhli
	zásuvný modul pro dynamické trasování do kompilátoru \acrlong{ghc},
	na\-implementovali prototyp a ukázali jeho schopnost zachytit klíčové informace
	o využití lenosti v jednoduchých Haskell programech.}
\placeForDeclarationOfAuthenticity{Prague}
\keywordsCS{Haskell, dynamické trasování, líné vyhodnocování, zásuvné moduly
kompilátorů, generické programování}
\keywordsEN{Haskell, dynamic tracing, lazy evaluation, compiler plugins,
generic programming}
\declarationOfAuthenticityOption{4} %select as appropriate, according to the desired license (integer 1-6)
% \website{http://site.example/thesis} %optional thesis URL


\begin{document}

\setsecnumdepth{part}
\chapter{Introduction} \label{sec:intro}
Conventional programming languages of all paradigms use -- almost equivocally
-- eager evaluation strategies. Non-strict semantics have far-reaching
implications on the design of a language\cite{haskell-is-pure} and pose many
implementation challenges.

Lazy evaluation is a potentially powerful implementation strategy
for non-strict languages, freeing the programmer to focus on what a program
means rather than on how it is computed. Laziness naturally accommodates
user-defined control flow and evaluates only the required subset of a given
program in a demand-driven manner. However, delayed evaluation makes
complexity analysis challenging and can lead to hard-to-predict memory
behaviour. To better understand the trade-offs laziness offers and how it
is used in practical scenarios, we design a dynamic tracing plugin for
the \acrlong{ghc}, implement a proof of concept, and demonstrate its
ability to record crucial information about the use of laziness in simple
Haskell programs.


\setsecnumdepth{all}
\chapter{State-of-the-art} \label{sec:state-of-the-art}
Although there are many functional languages of the ML family which enjoy
widespread use (F\#, OCaml, SML), Haskell is the only non-strict language among
them. Its most popular compiler\citationNeeded, the \acrfull{ghc}, implements
Haskell's non-strict semantics by lazy evaluation facilitated mainly by a
runtime data structure called a \textit{thunk}, which represents delayed
computations.

\todo{rewrite the following (repetitive use of ``although'', laziness is only
an implementation technique)}

Although necessary for non-strictness as required by the Haskell
spec\citationNeeded, laziness leads to many issues with runtime behaviour of
Haskell programs. The accumulation of thunks at runtime is a frequent cause of
pathological memory behaviour and unpredictable performance. There is a number
of libraries and tools which aim to help the Haskell programmer inspect the
runtime state of the Haskell heap, force the evaluation of thunks known to be
forced by the program at a later point anyway, and avoid their creation
altogether for certain expressions.

Among the surveyed approaches to the inspection and management of thunks were
the following:
\begin{itemize}
	\item Hoed
	\item \hackage{nothunks}
	\item Hat
	\item \hackage{htrace}
	\item \hackage{ghc-heap-view}
\end{itemize}

\section{Existing tools} \label{sec:existing-tools}
Several tools related to tracing are available.
\todo{...}

\subsection*{Hoed} \label{sec:hoed}
Hoed\cite{gh-hoed} is a tracer and a debugger for Haskell. Unlike the built-in
debugger of \acrshort{ghci}, Hoed is implemented as a regular Haskell library.
Users of Hoed manually annotate functions of interest to make the tracer
capture relevant information during execution. The annotations are simply calls
to the provided debugging function \hsIdent{observe} with a signature similar
to that of the \hsIdent{trace} function from the \hsModule{Debug.Trace} module
of Haskell's standard library, hiding unsafe IO. \hsIdent{observe} has type
\hsSignature{Observable a => Text -> a -> a}, its \hsType{Text} argument has to
equal the name of the function being annotated. The \hsType{Observable}
constraint on \hsType{a} is used by Hoed internally, the typeclass has a
default implementation. The resulting trace of the debugging session is exposed
via a web-based interface, to which the users connect with a regular web
browser. Hoed's traces include information about which functions have been
called during the execution of the annotated program and what were their
arguments. It only collects information about annotated functions.

Hoed features several tools to help users analyse problems with their code and
find the culprits of test failures. One of these is \textit{algorithmic
debugging}, an interactive trace browser which uses an algorithm similar to
binary search to locate the deepest incorrect function in the recorded call
tree. It does so by asking the user questions about whether certain evaluations
were correct, working its way gradually deeper into the tree. The ``algorithmic
debugger'' ultimately reports the faults it located.

While Hoed's approach to debugging is certainly interesting and quite far
removed from the concept of debuggers in other languages, it lacks any kind of
awareness of the low-level details of non-strictness. This is perhaps due to
the fact that it was implemented at a time when it was generally believed that
competing implementations of Haskell will emerge\citationNeeded.  Hoed is thus
intended for use with property testers like QuickCheck\citationNeeded, and not
as a tool for the identification and resolution of language implementation
-dependent issues, such as memory leaks.


\subsection*{\hackage{nothunks}} \label{sec:nothunks}
\hackage{nothunks} is a recently released Haskell package which helps in writing
thunk-free code. It defines a new typeclass, \hsTC{NoThunks}, along with
instances for common Haskell types. Any type with a \hsTC{NoThunks} instance
can be inspected for unexpected thunks. The library also implements a number of
alternatives to common functions from the prelude. These re\-implementations
check for unexpected thunks introduced during execution, throwing an exception
whenever a thunk is detected.

The exceptions of \hackage{nothunks} contain helpful information about the
context of the thunk which the library function detected, guiding the
programmer in locating the unexpectedly lazy code or data structure. The
library also allows various relaxations to the strictness of its inspection
policy, such as the \hsType{OnlyCheckWhnf} and \hsType{AllowThunk}
\hsCode{newtype}s. Thanks to GHC Generics\citationNeeded, \hackage{nothunks}
also offers the convenient \hsCode{deriving (Generic, NoThunks)} syntax to add
instances of the necessary typeclasses for custom data structures
automatically.
% TODO mention that nothunks works wonders for dealing with memory leaks, but
%     is aimed primarily at avoiding thunks altogether, not at their close
%     inspection or something like strictness analysis.

% TODO do look into the mechanism by which nothunks actually checks for thunks,
%      however

\subsection*{Hat} \label{sec:hat}
The Haskell Tracer Hat\cite{proj-hat} is a source-level tracer. It works by
compiling Haskell source files to annotated -- but still textual -- Haskell
source files. After this source-to-source translation, the user compiles the
annotated source code and runs it to produce a Hat trace.

The trace is a rich recording which contains high-level information about each
reduction the program performed. Hat comes with a number of utilities for
exploring the trace files, including some forms of forward and backward
debugging, filtering utilities which show all arguments passed to top-level
functions, virtual stack traces, and even an interactive tool for locating
errors in a program, similar to one of the features of Hoed.

\todo{Rewrite comment into text, scratch the paragraphs below}
% TODO so initially it was developed for the nhc compiler, gaining Haskell 98
% features and GHC support later on. The history of haskell paper
% https://dl.acm.org/doi/pdf/10.1145/1238844.1238856 has more info on this in
% section 10.4.2. Even in its prime time (?) it wasn't recognised as a useful
% tool, despite its many features.

The architectural decisions of Hat reflect the environment it originated in,
which unfortunately differs substantially from the current status quo. Its
source-to-source model of operation makes it compatible with various Haskell
compilers,


The Glasgow Haskell Compiler is the most widely used Haskell compiler
\citationNeeded with many language extensions beyond Haskell 2010. In 2009,
\acrshort{ghc} became the official compiler of the Haskell
Platform\cite{haskell-platform}, further cementing its monopoly as the primary
implementation of the language.

Hat uses the \hackage{haskell-src-exts} package to parse the source language.

% TODO \texttt{haskell-src-exts} DOES NOT lack feature parity with GHC
% TODO oblivious to laziness, implementation-agnostic

\subsection*{\hackage{htrace}} \label{sec:htrace}
\hackage{htrace} \citationNeeded is a simple package which exports a single
function: \hsSignature{htrace :: String -> a -> a}. As the name and function
signature suggest, this function mirrors the behaviour of the standard
\hsIdent{trace}, except that when displaying the tracing messages,
\hackage{htrace} shows them hierarchically indented based on the current call
depth. It works simply by manipulating a global mutable variable and hiding
this fact from the user with \hsIdent{unsafePerformIO}.

Although very simple and oblivious to any laziness implementation details, this
approach is still useful for debugging purposes. The indented tracing messages
suggest the depth to which various thunks are evaluated at different points of
the program's operation.

\subsection*{\hackage{ghc-heap-view}} \label{sec:ghc-heap-view}
\hackage{ghc-heap-view} is a Haskell package which makes advanced introspection
of the Haskell heap a possibility from within pure Haskell code. It relies on
the \hackage{ghc-heap} library which comes bundled with \acrshort{ghc}.

The library's notable high-level features include a function which attempts to
recreate readable Haskell source code from a runtime value, using \hsCode{let}
bindings to express sharing. There are also tree and graph data structures for
heap mapping and a high-level algebraic data type for all Haskell closures,
complete with their info tables.

\todo{to-do
\begin{itemize}
	\item explain trade-offs with those that need code changes (typeclass-based)
	\item explain problems with approaches independent of \acrshort{ghc}
\end{itemize}
}

Despite Haskell users' considerable interest in avoiding the implicit delaying
of computations which the language is notorious for, there are no records of a
large-scale study of the use of laziness in practice akin to
\cite{emp-study-laziness-r}. The tool with a feature set closest to what is
necessary for a comprehensive analysis of the practical use of laziness is
likely \hackage{ghc-heap-view}, which allows the user to interactively inspect
the heap objects and look inside thunks using \acrshort{ghci}. However, the
package primarily provides a rich library interface. It does not implement a
tracing mode, which would facilitate collection of laziness-relevant
information during the execution of entire programs.

\subsection{Summary} \label{sec:summary}
Table \ref{tbl:thunk-manager-comparison} summarizes the surveyed tooling.

% \begin{savenotes} % support for footnotes within a tabular environment
\begin{table}[h]
	\centering
	\begin{tabularx}{\textwidth}{|| l *{3}{L} >{\raggedright\arraybackslash}X ||}
		\hline
		Tool
		& Source changes
		& Order of evaluation
		& Thunks
		& Memory awareness\footnote{The surveyed programs span several layers
			of abstraction, with tools such as \nameref{sec:hoed} being completely
			oblivious to implementation details, \nameref{sec:nothunks}
			dependent on the runtime representation of values but not directly
			exposing it to the user, and \nameref{sec:ghc-heap-view} reifying
			implementation details.}
		\\ \hline \hline

		\nameref{sec:hoed}
			& Required    % source changes
			& Recorded    % order of evaluation
			& Transparent % thunks
			& None        % memory awareness
			\\ \hline
		\nameref{sec:nothunks}
			& Required    % source changes
			& Ignored     % order of evaluation
			& Detected    % thunks
			& Limited     % memory awareness
			\\ \hline
		\nameref{sec:hat}
			& Unnecessary % source changes
			& Recorded    % order of evaluation
			& Transparent % thunks
			& None        % memory awareness
			\\ \hline
		\nameref{sec:htrace}
			& Required    % source changes
			& Illustrated % order of evaluation
			& Transparent % thunks
			& None        % memory awareness
			\\ \hline
		\nameref{sec:ghc-heap-view}
			& Unnecessary\footnote{\nameref{sec:ghc-heap-view} does not require
				any changes to code which allocates the closures it is able to
				inspect.}
				% source changes
			& Ignored     % order of evaluation
			& Reified     % thunks
			& Full        % memory awareness
			\\ \hline
	\end{tabularx}
	\caption{An overview of existing solutions to thunk discovery and laziness
	debugging.}
	\label{tbl:thunk-manager-comparison}
\end{table}
% \end{savenotes}

\section{Existing profilers} \label{sec:existing-profilers}
\todo{...}

\subsection{Haskell Program Coverage} \label{sec:hpc}
Haskell Program Coverage\cite{hpc-paper} is (unsurprisingly) a code coverage
tool for Haskell. Similarly to Hat, \acrshort{hpc} has a source-to-source mode
of operation but additionally offers tight integration with \acrshort{ghc} and
comes bundled with modern releases of the compiler. It supports all
\acrshort{ghc} language extensions.

\acrshort{hpc} allows easy instrumentation of arbitrarily complex Haskell
programs without source annotations. It wraps subexpressions in the program
with an unsafe side-effecting function which records its evaluation by mutating
a module-wide array of integer counters. The final state of the per-module
arrays forms the \acrshort{hpc} trace. This architecture is wired into the
\acrshort{ghc} compiler pipeline in all the major data structures (the surface
syntax, Core language, and \acrshort{stg}), which makes it both robust and
per\-for\-mant. The tool comes bundled with utilities for displaying the
original source code with colourful mark-up, highlighting interesting
subexpressions based on the information extracted from the trace. Notably,
\acrshort{hpc} supports traces of the boolean values of pattern guards, which
are added to the visualisation.

\acrshort{hpc}'s feature set can be of tremendous help to the Haskell
programmer, especially when combined with tools like
QuickCheck\cite{quickcheck-paper}. However, its traces are tuned specifically
for code coverage and do not contain enough information to be useful for any
kind of dynamic strictness analysis. While the \acrshort{hpc} traces are
sufficiently granular, the subexpression counters lack necessary information
about their execution context and timing.


\section{The Glasgow Haskell Compiler} \label{sec:ghc}
\todo{Explain that several previous approaches had a direct influence on the
compiler. \acrshort{ghc} now has \acrshort{hpc}-specific features, compiler plugins, which
supersede source-to-source transformations (strengthening the monopoly but
simplifying implementation and streamlining the process), and a codebase that's
increasingly amenable to various extensions via the trees that grow pattern,
\hsIdent{Tickish}, etc.}
...

\subsection{Compiler plugins} \label{sec:compiler-plugins}
...


\chapter{Analysis and design} \label{sec:analysis-design}
\todo{what is analysis?}
...

\section{Analysis} \label{sec:analysis}
\begin{itemize}
	\item the core question: is laziness worth the hassle?
	\item original approach by modifying \acrshort{ghci}
	\item reuse the implementation of breakpoints, effectively implementing
		fairly clean hooks (bearing a slight resemblance to how the R tracer
		works)
	\item build on \texttt{Tickish}
\end{itemize}

\subsection{The problem} \label{sec:the-problem}
\todo{really? ``The problem''?}

The non-strict semantics of the Haskell language were a guiding principle which
influenced or directly determined many of the decisions made at its
inception\cite{history-of-haskell}. However, the implementation of non-strict
features via laziness in \acrshort{ghc} brings many pitfalls which Haskell
programmers need to deal with. Automatic avoidance of unnecessary thunk
allocations is conservative\citationNeeded: if \acrshort{ghc} is unable to
prove the strictness of a function in an argument by static strictness
analysis, the function will remain lazy, often leading to pathological memory
behaviour at runtime.

\todo{it's not just bang patterns though, the strictness annotations in
datatypes are a part of Haskell 2010!}
A simple and popular method of dealing with undesired laziness is the language
extension \texttt{BangPatterns}\citationNeeded\todo{cit. both for the popularity
and for the extension itself}. \texttt{BangPatterns} introduce a new syntax for
forcing an expression to \acrshort{whnf} when pattern-matching on
it\todo{clarification needed}.

\todo{explain that bang patterns are often only added after something goes
	wrong and the compiler doesn't help with choosing where to place them (or
	indeed with any memory ``leaks'' at all). Haskell keeps making things lazy
	and the developer keeps running into various bugs due to laziness, in a
	kind of whack-a-mole game (although that isn't exactly right, fixing one
	memory leak doesn't introduce another). Note also that Idris, a language
	inspired by Haskell, chose purity but not laziness.}

This fight against the semantics is detrimental to the developer experience of
the language. The question arises whether the benefits of laziness outweigh the
toll it takes on the programmer. To answer this question, the runtime behaviour
of lazy features needs to be understood. As a first step towards that
understanding, we design a dynamic tracing tool capable of capturing
information about the runtime behaviour of non-strict functions.

\subsection{Approach} \label{sec:approach}
\begin{itemize}
	\item core question: how is laziness used in practice?
	\item to understand that, we have to find out whether functions are strict and
		to what extent, discover potential strictness dependencies between their
		arguments, etc
	\item to do that, we need to determine whether an argument has been
		evaluated during function application
	\item to do \textit{that}, we have to look at runtime values
	\item to put the observations of runtime representations into context, we
		have to somehow keep track of function calls
\end{itemize}

The goal of this work is to design and implement a tool suitable for
understanding \todo{iffy} how is laziness utilised in real-life Haskell
programs. To analyse the practical implications of \acrshort{ghc}'s
implementation of non-strictness, we have to understand the strictness
properties of functions. For example, some arguments may be evaluated if and
only if others are. The tool must capture these dependencies and usage
patterns, as they may uncover both use cases where laziness is essential and
places where it could be safely avoided, even though static analysis cannot
determine so.

Dynamically inferring the strictness properties of functions requires a peek
under the hood of Haskell's runtime machinery. Typical Haskell code is
oblivious to the underlying representation of the values it manipulates, as
reification of thunks would weaken equational reasoning and parametricity.

\todo{nope, this is a false dichotomy}
There are two general approaches one could take to capture the information
about runtime structures over the execution of a Haskell program: modify the
program, or modify the compiler. The former would involve rewriting the source
code, while the latter

The purpose of the
project thus dictates use of features which violate some of the abstractions
provided by the language\todo{does it really? We're interested in GHCi as
well...}.

Understanding the use of laziness at runtime requires insight about runtime
structures that are otherwise transparent to the Haskell programmer. A key
feature of the language is its support for equational reasoning, which would be
broken if thunks were directly observable. To determine whether certain values
have been evaluated or not, we need to observe state that is typically hidden
from a Haskell program.

Once we have the power to inspect the runtime representations of values, we
need to use it to determine the strictness of functions. A function \hsCode{f}
is strict in an argument \hsCode{a} if \hsCode{a} has to be evaluated whenever
\hsCode{f a} is evaluated.

\subsection{The \acrshort{ghci} approach} \label{sec:ghci-approach}
Taking inspiration from \cite{emp-study-laziness-r}, the original
implementation plan was to work with \acrfull{ghci}. The bytecode compiler and
interpreter lack support for certain \acrshort{ghc} language extensions, namely
un\-box\-ed tuples and sums, but the supported subset of the language was
considered large enough to contain interesting examples\todo{maybe
``specimens''?}. The relative simplicity of the bytecode compilation pipeline
and the fairly straightforward evaluator were considered to provide a
foundation amenable to low-level tweaks deemed necessary for the extraction of
crucial tracing information.

The framework of the interpreter would ease the implementation of certain
features. \acrshort{ghci} already implements breakpoints, which pause the
execution of a Haskell program running in a separate thread and pass messages
to the controlling Haskell thread.

\subsubsection{\acrshort{ghci}: a primer}
\acrshort{ghci} is an interactive interface built on \acrshort{ghc}'s bytecode
compilation pipeline and the bytecode interpreter of the \acrshort{rts}.
\acrshort{ghci} offers a read-eval-print loop popular in other programming
languages.

\acrshort{ghci} consists of several key components: the \acrshort{ghci} UI, the
\acrshort{ghci} debugger, the bytecode generator, and the bytecode interpreter.
The former two are a part of the front end of \acrshort{ghc}, while the
bytecode-centric parts fit into the back end of the compiler pipeline and the
\acrshort{rts}, respectively.

The following sections will introduce each of the building blocks from which
\acrshort{ghci} is composed, starting with an overview of how they fit
together.

\subsubsection{The life of an interpreted expression}
The user's expression entered at the \acrshort{repl}'s prompt is fed through a
modified \acrshort{ghc} pipeline, as \acrshort{ghci} expects Haskell
expressions, not top-level definitions. This modified pipeline culminates in
bytecode generation, producing a collection of bytecode objects together with
high-level information about breakpoints, pointers to allocated string
literals, and other data.

The bytecode objects form, together with other information, a compiled
module\todo{is this correct? don't modules exist right after renaming?}.
That module is loaded by the compiler instance and \todo{uhhh...}

When evaluating an expression, the UI forks a new thread to perform evaluation
independently of the interface. This ensures that exceptions raised during
evaluation of an expression don't crash \acrshort{ghci}. The UI forwards
exception handlers appropriately to ensure this is the case. The two threads
communicate via \textit{mutable variables}, or \hsType{MVar}s. These are
concurrency primitives from the \hsModule{Control.Concurrent.MVar} module which
effectively implement concurrent, mutable
\hsType{Maybe}s\cite{concurrent-haskell}. A mutable variable of type
\hsType{MVar a} contains either no values or a single value of type \hsType{a}.
It can be safely shared across threads and supports operations
\hsIdent{takeMVar} and \hsIdent{putMVar}. The former operation extracts the
value stored in an \hsType{MVar}, leaving the variable empty if a value is
present. If the variable is empty, the operation blocks. The converse\todo{is
this the right word?} operation \hsIdent{putMVar} blocks on a full variable and
fills it with a value as soon as it is empty.

Two \hsType{MVar}s play an important role in the design of \acrshort{ghci},
\hsIdent{statusMVar} and \hsIdent{breakMVar}. These variables form a
communication channel between the UI thread and the thread responsible for the
evaluation of an expression.

\todo{either reuse for the UI section or get rid of this}
They are greeted with the interpreter's UI\todo{is it necessary
to include this in the acronyms?} and can begin writing Haskell expressions
directly or first invoking various \acrshort{ghci} commands to load modules,
print types, kinds, and documentation, browse the contents of modules and
perform other tasks.

\subsubsection{Bytecode generation}
The bytecode facilities of \acrshort{ghc} involve a detour from the typical
sequence of steps performed to transform Haskell sources all the way to a form
suitable for linking or execution. After desugaring, the program is transformed
directly into bytecode instructions\footnote{Note that this approach will soon
be replaced by a new bytecode pipeline which follows the usual compilation
process all the way to \acrshort{stg}\citationNeeded.}. Optimisations
implemented in the simplifier are not performed. \acrshort{ghci} is intended
for interactive evaluation and favours fast, iterative development over runtime
performance, making the naive code generation approach a reasonable choice.

Every top-level definition, every scrutinee of a \hsCode{case} expression, and
every right-hand side of a non-trivial \hsCode{let} expression are compiled to
a \acrfull{bco}. Such an object contains an array of bytecode instructions
together with \todo{finish this}

The bytecode format comprises 67 instructions
\todo{describe the instructions in a table}
\todo{stack/heap checks prevent (uh, or maybe react to?) respective overflows}

\subsubsection{The bytecode interpreter}
The interpreter which \acrshort{ghci} relies on is a part of the
\acrshort{rts}. Its primary workhorse is the \texttt{interpretBCO} function
which handles closure evaluation, unboxed returns, function application, and
interpretation of bytecode instructions. For tasks it is unable to deal with,
such as application of machine-code functions, it returns to the scheduler.

Interpretation works simply by case analysis on the current instruction.

\subsubsection{The debugger}
A notable feature of the tool is the \acrshort{ghci} debugger, which allows the
programmer to place breakpoints on certain expressions in their code. The
interpreter then pauses execution when it is about to evaluate an expression
marked by a breakpoint.

Due to laziness, the order in which breakpoints are hit depends on the order in
which their respective thunks are forced to \acrshort{whnf}, not directly on
the order in which functions are called. Breakpoints thus equip the Haskell
programmer with a powerful tool for debugging order of evaluation issues caused
by the language's non-strict semantics.

Internally, breakpoints rely on a special bytecode instruction called
\texttt{BRK\_FUN}. Upon encountering this instruction, the interpreter first
checks whether it is already returning from a breakpoint (via a flag in the
\acrshort{tso}). If it is not returning from a breakpoint and the associated
breakpoint is enabled, the interpreter pauses execution at this point.

Pausing on a breakpoint is quite an involved action. The interpreter prepares
to call an ``IO action,'' which is a Haskell function invoked to resume
\acrshort{ghci}'s UI thread by filling the shared mutable variable. This
preparation saves the top stack frame to a new closure, a pointer to which is
passed to the IO action. The stack is then set up to call the IO action, and
the interpreter returns to the scheduler in order to perform the call.

At no point is the instruction pointer persisted -- the progress of evaluation
of the current \acrshort{bco} is lost whenever the interpreter stops at a
breakpoint. This is acceptable, as the bytecode generator makes sure to only
put \texttt{BRK\_FUN} instructions at the very start of bytecode objects and
the \acrshort{tso} flag ensures that a just-visited breakpoint is not stopped
at again.

\subsubsection{The user interface}
\ldots

\subsection{The compiler plugin approach}
To produce useful tracing output, a dynamic tracing framework must capture
interesting events during a program's evaluation and relate them to one
another. In particular, the evaluation of function arguments must be clearly
related to the respective function call to enable reasoning about the
strictness of a function on a call-by-call basis. While retaining the order of
evaluation is trivial in a call-by-value language, laziness introduces
interleaving. This can only be dealt with by the introduction of state into the
program (or into the tracing framework) in order to recover the dependencies
between function calls and argument evaluations, which are no longer implicit
in the order of the trace events.

It is this function-call-specific state that becomes difficult to express
without high-level information about the program structure at hand, as was the
case with \nameref{sec:ghci-approach}\todo{lowercase}.

\subsubsection*{Adding state}
\todo{A manual introduction of state into the program is rather trivial\ldots}

Fortunately, function-call-specific state can be easily introduced into the
source program, simply as local variables. It suffices to keep a unique
identifier of the particular function call that the argument evaluation traces
can refer to. Such a unique identifier necessarily needs to change with every
function call. In clean Haskell code without unsafe features, this is
impossible in general, as the language requires the use of the \hsType{IO}
monad in order to perform side-effecting computations.

Since rewriting functions into a monadic form would be a difficult undertaking,
we prefer the way of unsafe features. Integer counters are enough for call
identification purposes, so we choose to keep one counter per function. All
counters can be stored in a single mutable map, which associates

\todo{
to-dos:
\begin{itemize}
	\item describe the general idea of the source plugin
	\item explain the use of SYB
	\item explain the use of TH and splicing
	\item explain how the global mutable map resides in the plugin's module
	\item explain the difficulty of trying to implement similar functionality
		in a core plugin instead
\end{itemize}
}

Equipped with a means of introducing benign side-effects into programs for
tracing purposes, we are in search of a way of rewriting source code to put
these side-effects to use. One plausible approach would be direct source code
rewriting, akin to \nameref{sec:hat}. As described in section
\ref{sec:existing-tools}, source-to-source transformations have the benefit of
generality, but also the downside of additional complexity in both the
rewriting process itself and the build process of the program, which the user
of our tool would have to deal with. Furthermore, true implementation
agnosticism of the tracing framework would require compiler-independent support
for inspection of the Haskell heap, for which no solution seems to exist at the
time of writing\todo{right?}. A less general but more ergonomic way of
rewriting source code is via \acrshort{ghc}'s \textit{source plugins}, which
hook directly into the compiler pipeline and can operate on the surface-level
syntax at different stages.

\subsubsection*{Source plugins}
Source plugins\cite{ghc-source-plugins} are a relatively recently introduced
feature of \acrshort{ghc}. Compiler source plugins are Haskell packages which
invoke the \acrshort{ghc} API\todo{same as UI, I suppose?} to hook into the
compiler pipeline and modify the compiled program at various stages of the
front-end. Unlike Core plugins\citationNeeded, which operate on the internal
language, source plugins deal with the entirety of Haskell's surface syntax.

Rather than parsing, transforming, and serialising the source code separately
to the compilation step, we can design a plugin that performs the required
source transformations in the compiler pipeline directly. We introduce two
tracing functions, \hsIdent{traceEntry} and \hsIdent{traceArg}, into the
current module. We then rewrite the source program to call \hsIdent{traceEntry}
every time a function in the program is invoked and we thread every reference
to a function's argument through \hsIdent{traceArg}. This introduces the
opportunity to inspect the runtime representations of the arguments passed to a
function when the result of the function is under scrutiny.

We can determine the strictness properties of a transformed function from the
calls it makes to the tracing utilities. If we record a call to a (transformed)
top-level function \hsCode{f :: Int -> Int -> Int} defined as \hsCode{f x y =
...} via \hsIdent{traceEntry} but no calls to \hsIdent{traceArg}, the
function makes no use of any of its arguments, and is therefore non-strict in
both of them. Examples of functions of this behaviour include \hsCode{f x y =
3}, \hsCode{f x y = undefined}, or \hsCode{f x y = f x y}. Note that the latter
example references the arguments on the \acrshort{rhs}, but these references are
never evaluated. If a call to \hsIdent{f} is followed by a call to
\hsIdent{traceArg} for the \hsIdent{x} argument, but the program terminates and
no calls to \hsIdent{traceArg} for the \hsIdent{y} argument occur, we say that
\hsIdent{f} is strict in \hsIdent{x} and \textit{potentially non-strict} in
\hsIdent{y}. \hsIdent{f} could be non-strict in \hsIdent{y}, but it could also
conditionally require \hsIdent{y} to be evaluated based on the value of
\hsIdent{x}. The property of a multi-argument function being strict in one
argument if another argument matches a predicate (and being non-strict in that
argument otherwise) is what makes the interpretation of traces of nested
functions tricky.

\subsubsection*{Rewriting the \acrshort{ast}}
Armed with the necessary tracing functions and a plan on how to apply them, we
move on to the problem of syntax tree transformation. The \texttt{GhcPlugins}
module\cite{hkg-ghcplugins} of the \acrshort{ghc} \acrshort{api} includes the
necessary functions to hook into the compiler pipeline. A source plugin can
choose to modify the syntax tree at three different stages: right after
parsing, between renaming and typechecking, or just after the typechecker has
run. These hooks involve different trade-offs. Construction of new (sub)trees
becomes more and more difficult further down the pipeline as the internal
representation accumulates metadata from the various stages. On the other hand,
the available metadata may be necessary for certain tasks and can help plugin
authors write more robust implementations. For example, constructing parsed
expressions is almost as easy as writing the surface syntax in a source file,
using strings as identifiers, but it may result in accidental captures of
bindings in scope. Because the renaming phase disambiguates identifiers,
constructing renamed \acrshort{ast}s avoids this issue, at the expense of
either working with abstract identifiers, or invoking a renaming phase
manually.

\todo{fix \texttt{\textbackslash citeauthor}}
As \citeauthor{blog-source-plugins}'s introduction to source plugins shows, the
costs associated with the construction of syntax trees later in the pipeline
are not prohibitive\cite{blog-source-plugins}. The \acrshort{ghc}
\acrshort{api} exports high-level functions which let the plugin author take
trees from parsed to renamed to typechecked in only a few lines of code.
Moreover, the plugin author can use the quasi\-quoting\cite{th-quasiquoting}
features of Template Haskell\cite{th-classic} to greatly simplify the
construction of expressions. The quasi\-quoting facilities even manage
references to definitions in the scope of the plugin's source code
automatically. Common patterns in the expressions created by the plugin can be
included as regular top-level definitions in the plugin's module or in a module
the plugin depends on and spliced into the syntax tree. With these high-level
features in mind, the suitable injection mechanisms for a dynamic tracing
source plugin seem to be before and after typechecking. We only discuss the
latter approach in the following text, even though a source plugin operating on
the renamed \acrshort{ast} would likely be very similar. Note that the
\acrshort{api} makes no hard distinction between the different approaches to
pipeline extensions. Indeed, a source plugin simply provides a value of the
\hsType{Plugin} data type, overriding the appropriate fields of a default
plugin implementation with monadic functions. A source plugin could run custom
code after each of the frontend stages.

The actual process of rewriting the right-hand sides of function definitions
involves the data types for the surface syntax of Haskell, which has hundreds
of constructs\cite[Key~Design~Choices]{arch-ghc}. The general task of
transforming hierarchies of deeply nested data types has many innovative
Haskell solutions, including optics and generic programming. While we could use
pro\-functor optics or novel generic approaches, we leverage a fairly simple,
if a bit dated, generic programming technique via the \acrfull{syb}
library\cite{syb-paper}. \acrshort{syb}'s built-in querying and transformation
schemes empower the Haskell programmer with means of applying type-specific
functions in all appropriately typed fields of a nested data structure. The
library is built using powerful generalisations of folding and a number of
combinators, making it easy to create new traversal schemes as compositions of
existing building blocks.

\subsubsection*{Anatomy of a plugin}
Our source plugin consists of four modules.
\begin{itemize}
	\item \hsModule{TracingPlugin}, the entry point of execution and the only
		exposed module of the package,
	\item \hsModule{Typechecking}, which contains utilities for typechecking
		expressions constructed by the plugin,
	\item \hsModule{Logging}, which defines the tracing functions that we
		compile into source programs, and
	\item \hsModule{Rewriting}, where the magic happens. \todo{perhaps
		unprofessional}
\end{itemize}

The \hsModule{TracingPlugin} module simply defines and exports a
\hsType{Plugin} derived from the \hsIdent{defaultPlugin} implementation,
overriding \hsIdent{typeCheckResultAction}, the function invoked after the
typechecking phase. Neglecting command-line arguments, our action has the type
\hsType{ModSummary -> TcGblEnv -> TcM TcGblEnv}. As the type indicates, it
computes within the typechecking monad (\hsType{TcM}) with access to
information about the current module (\hsType{ModSummary}), modifying its
typechecking environment (\hsType{TcGblEnv}). The action is invoked once for
each compiled module. The typechecking environment is a large data structure
which describes the top level of a module with 58 fields. Of these, only
\hsCode{tcg_binds :: LHsBinds GhcTc} is interesting to us. The type
constructor \hsType{LHsBinds} stands roughly for ``located Haskell bindings''
and represents a collection of all the top-level bindings of a module annotated
with their source file locations. Our post-typechecking action simply threads
this field through our rewriting function, which also computes in the
typechecking monad, and returns the transformed bindings.

The rewriting function, shown in figure \ref{fig:hs-rewrite}, resides in the
\hsModule{Rewriting} module. It initiates a stateful computation which
transforms the bindings in a generic manner using the \acrshort{syb} library.
\begin{figure}[h]
	\centering
	\begin{minted}[autogobble]{haskell}
        rewrite :: LHsBinds GhcTc -> TcM (LHsBinds GhcTc)
        rewrite binds = fst <$> (`runStateT` initialState)
                                (everywhereM' trans binds)
	\end{minted}
	\caption{The top-level rewriting function, a sole export of the
	\hsModule{Rewriting} module.}
	\label{fig:hs-rewrite}
\end{figure}

Since the \acrshort{ghc} \acrshort{api} abstracts over compiler state using
(among other types) the \hsType{TcM} monad, the generic transformation
involving any non-trivial compiler computations needs to be monadic as well.
This transformation is implemented by the \hsIdent{trans} function (shown in
fig. \ref{fig:hs-trans}), which additionally carries a context from the roots
of the top-level definitions down to their leaves. We combine the stateful
traversal with the typechecking monad by way of the \hackage{mtl} package,
itself inspired by \cite{higher-order-polymorphism}, using the \hsType{StateT}
monad transformer.

\begin{figure}[h]
	\centering
	\begin{minted}[autogobble]{haskell}
        trans :: Typeable a => a -> StateT WrapperState TcM a
        trans = mkM collectFunInfo `extM` wrapRef `extM` incrementCC
	\end{minted}
	\caption{The generic transformation function.}
	\label{fig:hs-trans}
\end{figure}

\hsIdent{trans} is applied in a single, top-down traversal of the
\acrshort{ast}s via a \acrshort{syb} scheme derived from \hsIdent{everywhereM}.
Ultimately, the function pattern-matches on important structures in the syntax
trees of top-level bindings in three different ways:
\begin{enumerate}
	\item \hsIdent{collectFunInfo} adds information about the current function
		to the \hsType{WrapperState},
	\item \hsIdent{wrapRef} wraps argument references with a tracing function, and
	\item \hsIdent{incrementCC} wraps the right-hand side of each function with
		a \hsCode{let} binding, introducing a call counter variable into
		its scope.
\end{enumerate}

Each of these building blocks of the complete transformation operates slightly
differently.

\begin{description}
	\item[\hsCode{collectFunInfo :: Bind -> StateT WrapperState TcM Bind}]
		pattern-matches on the various sorts of bindings that can appear in an
		\acrshort{ast} and extracts the names of the named ones, saving them to
		the \hsType{WrapperState} context, thus providing the name of the
		innermost named function to the other transformations.

	\item[\hsCode{incrementCC :: RHS -> StateT WrapperState TcM RHS}]
		pattern-matches on right-hand sides of functions and introduces calls
		to the tracing function \hsIdent{traceEntry} using \acrfull{th}.
		Calling \hsIdent{traceEntry} with a function name increments a global
		call counter for that function and returns the counter's current value.
		\hsIdent{incrementCC} has to introduce a new binding in the scope of
		the right-hand side so that tracing calls on the \acrshort{rhs} can
		refer to the call ID.  Since \acrshort{th} cannot lift the Haskell
		\acrshort{ast} types, the binding has to be constructed in two steps.

		First we read the \hsType{WrapperState} to find out the name of the
		function we are currently transforming. We construct a \acrshort{th}
		expression for the application of \hsIdent{traceEntry} to the function
		name and bind it via a \hsCode{let} binding which assigns the result to
		a new call counter variable in the scope of a dummy expression (a proxy
		to \hsIdent{undefined}). Then we typecheck this expression and run a
		\acrshort{syb} transformation which replaces the dummy subexpression
		with the original right-hand side. Care must be taken when replacing a
		node in the typechecked \acrshort{ast} because the typechecker inserts
		type applications for polymorphic terms such as \hsIdent{undefined}.

		Finally, \hsIdent{incrementCC} also finds the \hsType{Id} of the call
		counter variable via a \acrshort{syb} query and saves it in the
		\hsType{WrapperState}.

	\item[\hsCode{wrapRef :: LExpr -> StateT WrapperState TcM LExpr}]
		pattern-matches on references to function arguments in function bodies.
		Its purpose is to transform every argument reference into a call to
		\hsIdent{traceArg}.

		\todo{fix \texttt{\textbackslash hsCode} overflows}

		To identify references to function arguments, \hsIdent{wrapRef}
		consults the \hsCode{boundVars :: [Id]} collection. This collection is built
		independently of the \hsIdent{wrapRef} transformation, since it needs
		no function-specific information. We rely on the fact that while
		references to bindings are semantically valid only in local
		(lexically-scoped) contexts, they have globally unique identifiers.
		Collecting the identifiers of function arguments is thus a simple task
		of traversing all the syntactical pattern-matching structures which
		bind them. We once again leverage \acrshort{syb} to do this without
		having to pattern-match on the entirety of surface syntax.

		When the reference wrapping transformation identifies a function
		argument, it constructs a partial application of the \hsIdent{traceArg}
		tracing function and applies the original binding reference to it. The
		partially-applied \hsIdent{traceArg} is an unsafe identity function
		which logs information about the argument's runtime representation to a
		file.

		Since the overall rewriting operation proceeds in a top-down manner,
		the \hsIdent{wrapRef} transformation runs into the issue of producing
		subexpressions it could recursively match on again. This could be
		avoided by tagging the transformed expressions somehow. Unfortunately,
		this is difficult to achieve, because the \acrshort{ast} datatypes lack
		useful typeclass instances for doing so. Crucially, there is no notion
		of equality on syntax trees and no hashing implementation which would
		let us store the transformed expressions in a hash set (or at least a
		set). We work around this limitation by stripping the source location
		tags from the \acrshort{ast} nodes and checking for their presence
		before invoking \hsIdent{wrapRef}'s rewriting logic, but we are aware
		of the problems with this approach. However, issues with error
		reporting are largely mitigated by the fact that the plugin is invoked
		after the source program passed the typechecking phase.
\end{description}

\subsubsection*{Implementation of tracing utilities} \label{sec:tracing-util-impl}
The tracing functions inserted into the \acrshort{ast} by the rewriting logic
reside in the \hsModule{Logging} module. They leverage the unsafe IO features
of Haskell, specifically the standard \hsCode{unsafePerformIO :: IO a -> a}
from \hsModule{System.IO.Unsafe}, to hide the side-effects of tracing from the
type system. When invoked, these functions append a row of
\acrshort{csv}-encoded data to a \textit{trace file}, a log of interesting
events that occurred during the evaluation of a Haskell program, which is
suitable for further analysis.

\begin{description}
	\item[\hsCode{traceEntry :: String -> Int}] marks the evaluation entry
		point of a function. Taking the function's name, it increments its call
		counter in the background and returns its new value. The call counters
		are stored in a global map called \hsCode{functionEntries :: IORef (Map
		String Int)}. The \hsType{IORef} indirection makes
		\hsIdent{functionEntries} a mutable variable which can be manipulated
		in the \hsType{IO} monad. The map is explicitly marked with a
		\mintinline[
			breakbytokenanywhere,breaklines
		]{haskell}|{-# NOINLINE #-}|
		pragma to ensure it is shared between the tracing calls. Since the
		\hsType{IORef} constructor returns a reference in the \hsType{IO}
		monad, we allocate the global variable via \hsIdent{unsafePerformIO}.

		The call counter map is empty at first, individual counters are
		initialised on-demand. The initialisation of a new call counter and the
		increment of an existing one are both described concisely by the
		\hsIdent{insertWith} operation on \hsType{Map}s, which takes a binary
		function on values, a key, an initial value, and a map, and either
		initialises the key to the initial value or updates it by applying the
		binary function to its current value and the initial one. This
		operation is applied atomically via \hsIdent{atomicModifyIORef'} to
		accommodate concurrent updates.

	\item[\hsCode{traceArg :: String -> String -> Int -> a -> a}] indicates a
		reference to a function argument. Partially applying this function to
		the name of the enclosing function, the name of the referenced
		argument, and the number of the call to the enclosing function leaves
		an impure identity, which is applied to the actual argument.
		\hsIdent{traceArg} leverages the \hackage{ghc-heap-view} library to
		take a peek at the runtime representation of the argument to determine
		whether it has been evaluated or not.

	\item[\hsCode{logt :: TraceSort -> [String] -> IO ()}] persists a tracing
		message to a shared file. Calls to this function are not introduced
		during the rewriting process directly, but both \hsIdent{traceEntry}
		and \hsIdent{traceArg} call it internally. The function can thus stay
		in the safe realm of the language, as its type indicates. File system
		operations in Haskell require a value of type \hsType{Handle} which the
		\acrshort{rts} uses to manage IO with file system objects. Allocating a
		handle corresponds to opening a file. Since that is a potentially
		expensive operation, we store it in an \hsType{IORef}, again created
		globally with \hsIdent{unsafePerformIO}. \hsIdent{logt} then simply
		reads the \hsType{IORef}, appends tracing data to the file, and flushes
		the handle, to avoid problems with lazy IO and data loss when the
		program exits.
\end{description}


\chapter{Realisation}
\ldots

\section{Development environment}
\ldots

\section{Building \acrshort{ghc}}

The \acrshort{ghc} codebase is a large and complicated collection of source
files written in a number of programming languages, primarily Haskell and
C\cite{arch-ghc}. The ever-evolving project is supported by a custom build
system called Hadrian\citationNeeded, itself written in Haskell, which
bootstraps the self-hosting compiler in several steps. To build \acrshort{ghc},
an appropriate version of \acrshort{ghc} has to be installed already. The
installed compiler is referred to as the \textbf{stage 0} compiler \todo{fix
the formatting of stages}. Hadrian uses the \textbf{stage 0} compiler to build
first the Hadrian build system and with it the \textbf{stage 1} compiler, which
is a freshly built \acrshort{ghc} linked against the \textbf{stage 0}
\hackage{base} library. The \textbf{stage 1} compiler is subsequently used to
build the core libraries from scratch. It is then utilised again to build the
\textbf{stage 2} compiler, which is linked against the freshly built
\hackage{base}. The \textbf{stage 2} compiler constitutes a complete build of
\acrshort{ghc} from source code. There is an optional follow-up step, where the
\textbf{stage 2} compiler builds a \textbf{stage 3} compiler, which is useful
for profiling \acrshort{ghc} while building \acrshort{ghc}.

The first step to working on the project after obtaining the source code is
setting up the build system. Since specific releases of \acrshort{ghc} require
specific \textbf{stage 0} compilers as the project quickly adapts to use new
language extensions, the management of \acrshort{ghc} versions on a Unix-like
system with a system-wide package manager can be difficult. To ease the
management of installed versions and enable quick switching between them, the
\texttt{ghcup} tool\cite{ghcup} has been developed. \todo{this will need some
more citations} \texttt{ghcup} lets the \acrshort{ghc} developer quickly
install and switch between the releases of not only \acrshort{ghc} itself, but
also Cabal, the Haskell build system and dependency manager, and the
\acrfull{hls}, an \acrshort{lsp}-compliant language server providing
Haskell-specific editor integration features.

There are several supported approaches to building \acrshort{ghc}, as the
compiler previously used a build system based on \acrshort{gnu} Make (before
switching to Hadrian) and the old Make build system is still being phased out.
Additionally, the build tool of the programmer's choice can be combined with a
Docker or Nix -assisted set-up, simplifying the installation of other
dependencies required for the build process.

\todo{how do we say ``let's not pick Make tho?''}
\todo{ways and flavours!}
After the initial build, the \textbf{stage 1} compiler can be \textit{frozen}
by passing a flag to the build system on subsequent invocations. This prevents
rebuilding the \textbf{stage 1} compiler every time a source file changes, which
speeds up the edit-compile-run cycle tremendously.

...



\setsecnumdepth{part}
\chapter{Conclusion}
...

\bibliography{./bbl.bib}
\bibliographystyle{iso690.bst}

\setsecnumdepth{all}
\appendix

\printglossary[type=\acronymtype]


\chapter{Contents of enclosed CD}

\todo{figure out what to do about this}
%change appropriately

\begin{figure}
	\dirtree{%
		.1 readme.txt\DTcomment{the file with CD contents description}.
		.1 exe\DTcomment{the directory with executables}.
		.1 src\DTcomment{the directory of source codes}.
		.2 wbdcm\DTcomment{implementation sources}.
		.2 thesis\DTcomment{the directory of \LaTeX{} source codes of the thesis}.
		.1 text\DTcomment{the thesis text directory}.
		.2 thesis.pdf\DTcomment{the thesis text in PDF format}.
		.2 thesis.ps\DTcomment{the thesis text in PS format}.
	}
\end{figure}

\end{document}
