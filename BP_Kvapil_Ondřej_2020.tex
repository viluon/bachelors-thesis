% arara: xelatex
% arara: xelatex
% arara: xelatex


% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=B,english]{FITthesis}[2019/12/23]

\usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8
% \usepackage[latin2]{inputenc} % LaTeX source encoded as ISO-8859-2
% \usepackage[cp1250]{inputenc} % LaTeX source encoded as Windows-1250

% \usepackage{subfig} %subfigures
% \usepackage{amsmath} %advanced maths
% \usepackage{amssymb} %additional math symbols

\usepackage{dirtree} %directory tree visualisation
\usepackage{xcolor}
\usepackage{blindtext}
\usepackage{footnote}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{booktabs}


% custom commands
\newcommand{\todo}[1]{\textcolor{red}{\textbf{[[#1]]}}}
\newcommand{\blind}[1][1]{\textcolor{gray}{\Blindtext[#1][1]}}
\newcommand{\citationNeeded}{\textcolor{red}{\textbf{[citation needed]}}}
\newcommand{\hackage}[1]{\texttt{#1}}
\newcommand{\hsSignature}[1]{\texttt{#1}}
\newcommand{\hsType}[1]{\texttt{#1}}
\newcommand{\hsIdent}[1]{\texttt{#1}}
\newcommand{\hsModule}[1]{\texttt{#1}}
\newcommand{\hsTC}[1]{\texttt{#1}}
\newcommand{\hsCode}[1]{\texttt{#1}}

% tabularx customisation
\newcolumntype{L}{>{\RaggedRight\arraybackslash}X}


% list of acronyms
\usepackage[acronym,nonumberlist,toc,numberedsection=autolabel]{glossaries}
\iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
\makeglossaries

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% EDIT THIS
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\department{Programming Research Laboratory}
\title{Haskell Dynamic Tracing}
\authorGN{Ondřej} %author's given name/names
\authorFN{Kvapil} %author's surname
\author{Ondřej Kvapil} %author's name without academic degrees
\authorWithDegrees{Ondřej Kvapil} %author's name with academic degrees
\supervisor{Ing. Filip Křikava, Ph.D.}
\acknowledgements{THANKS (remove entirely in case you do not wish to thank anyone)}
\abstractEN{Summarize the contents and contribution of your work in a few sentences in English language.}
\abstractCS{V n{\v e}kolika v{\v e}t{\' a}ch shr{\v n}te obsah a p{\v r}{\' i}nos t{\' e}to pr{\' a}ce v {\v c}esk{\' e}m jazyce.}
\placeForDeclarationOfAuthenticity{Prague}
\keywordsCS{Replace with comma-separated list of keywords in Czech.}
\keywordsEN{Replace with comma-separated list of keywords in English.}
\declarationOfAuthenticityOption{4} %select as appropriate, according to the desired license (integer 1-6)
% \website{http://site.example/thesis} %optional thesis URL


\begin{document}

% \newacronym{CVUT}{{\v C}VUT}{{\v C}esk{\' e} vysok{\' e} u{\v c}en{\' i} technick{\' e} v Praze}
% \newacronym{FIT}{FIT}{Fakulta informa{\v c}n{\' i}ch technologi{\' i}}
\newacronym{ghc}{GHC}{Glasgow Haskell Compiler}
% TODO: isn't there a better glossary package that handles references
% correctly?
\newacronym{ghci}{GHCi}{\acrshort{ghc} interpreter}
\newacronym{rts}{RTS}{\acrshort{ghc} RunTime System}
\newacronym{repl}{REPL}{Read-Eval-Print Loop}
\newacronym{ast}{AST}{Abstract Syntax Tree}
\newacronym{hpc}{HPC}{Haskell Program Coverage}
% TODO this should probs go into a glossary instead?
\newacronym{stg}{STG}{Spineless Tag\-less G-machine, an abstract machine based
on graph reduction\cite{stg-classic}. \acrshort{ghc} compiles the Core language
to STG instructions, machine code is generated from the STG representation.}
% TODO this as well (the acronyms should only list the unabbreviated form, not
% a description of what these are)
\newacronym{gnu}{GNU}{GNU's Not Unix, a Unix-like operating system}
\newacronym{hls}{HLS}{Haskell Language Server}
\newacronym{bco}{BCO}{Byte Code Object}
% TODO citation?
\newacronym{lsp}{LSP}{Language Server Protocol}
\newacronym{tso}{TSO}{Thread State Object}
\newacronym{whnf}{WHNF}{Weak Head Normal Form}


\setsecnumdepth{part}
\chapter{Introduction} \label{sec:intro}
\todo{...}


\setsecnumdepth{all}
\chapter{State-of-the-art} \label{sec:state-of-the-art}
Although there are many practical functional languages in the ML family (F\#,
OCaml, SML), Haskell is the only non-strict language among them. Its most
popular compiler\citationNeeded, the \acrfull{ghc}, implements Haskell's
non-strict features by lazy evaluation facilitated mainly by a runtime data
structure called a \textit{thunk}, which represents delayed computations.

\todo{rewrite the following (repetitive use of ``although'', laziness is only
an implementation technique)}

Although necessary for non-strictness as required by the Haskell
spec\citationNeeded, laziness leads to many issues with runtime behaviour of
Haskell programs. The accumulation of thunks at runtime is a frequent cause of
pathological memory behaviour and unpredictable performance. There is a number
of libraries and tools which aim to help the Haskell programmer inspect the
runtime state of the Haskell heap, force the evaluation of thunks known to be
forced by the program at a later point anyway, and avoid their creation
altogether for certain expressions.

Among the surveyed approaches to the inspection and management of thunks were
the following:
\begin{itemize}
	\item Hoed
	\item \hackage{nothunks}
	\item Hat
	\item \hackage{htrace}
	\item \hackage{ghc-heap-view}
\end{itemize}

\section{Existing tools} \label{sec:existing-tools}
Several tools related to tracing are available.
\todo{...}

\subsection*{Hoed} \label{sec:hoed}
Hoed\cite{gh-hoed} is a tracer and a debugger for Haskell. Unlike the built-in
debugger of \acrshort{ghci}, Hoed is implemented as a regular Haskell library.
Users of Hoed manually annotate functions of interest to make the tracer
capture relevant information during execution. The annotations are simply calls
to the provided debugging function \hsIdent{observe} with a signature similar
to that of the \hsIdent{trace} function from the \hsModule{Debug.Trace} module
of Haskell's standard library, hiding unsafe IO. \hsIdent{observe} has type
\hsSignature{Observable a => Text -> a -> a}, its \hsType{Text} argument has to
equal the name of the function being annotated. The \hsType{Observable}
constraint on \hsType{a} is used by Hoed internally, the typeclass has a
default implementation. The resulting trace of the debugging session is exposed
via a web-based interface, to which the users connect with a regular web
browser. Hoed's traces include information about which functions have been
called during the execution of the annotated program and what were their
arguments. It only collects information about annotated functions.

Hoed features several tools to help users analyse problems with their code and
find the culprits of test failures. One of these is \textit{algorithmic
debugging}, an interactive trace browser which uses an algorithm similar to
binary search to locate the deepest incorrect function in the recorded call
tree. It does so by asking the user questions about whether certain evaluations
were correct, working its way gradually deeper into the tree. The ``algorithmic
debugger'' ultimately reports the faults it located.

While Hoed's approach to debugging is certainly interesting and quite far
removed from the concept of debuggers in other languages, it lacks any kind of
awareness of the low-level details of non-strictness. This is perhaps due to
the fact that it was implemented at a time when it was generally believed that
competing implementations of Haskell will emerge\citationNeeded.  Hoed is thus
intended for use with property testers like QuickCheck\citationNeeded, and not
as a tool for the identification and resolution of language implementation
-dependent issues, such as memory leaks.


\subsection*{\hackage{nothunks}} \label{sec:nothunks}
\hackage{nothunks} is a recently released Haskell package which helps in writing
thunk-free code. It defines a new typeclass, \hsTC{NoThunks}, along with
instances for common Haskell types. Any type with a \hsTC{NoThunks} instance
can be inspected for unexpected thunks. The library also implements a number of
alternatives to common functions from the prelude. These re\-implementations
check for unexpected thunks introduced during execution, throwing an exception
whenever a thunk is detected.

The exceptions of \hackage{nothunks} contain helpful information about the
context of the thunk which the library function detected, guiding the
programmer in locating the unexpectedly lazy code or data structure. The
library also allows various relaxations to the strictness of its inspection
policy, such as the \hsType{OnlyCheckWhnf} and \hsType{AllowThunk}
\hsCode{newtype}s. Thanks to GHC Generics\citationNeeded, \hackage{nothunks}
also offers the convenient \hsCode{deriving (Generic, NoThunks)} syntax to add
instances of the necessary typeclasses for custom data structures
automatically.
% TODO mention that nothunks works wonders for dealing with memory leaks, but
%     is aimed primarily at avoiding thunks altogether, not at their close
%     inspection or something like strictness analysis.

% TODO do look into the mechanism by which nothunks actually checks for thunks,
%      however

\subsection*{Hat} \label{sec:hat}
The Haskell Tracer Hat\cite{proj-hat} is a source-level tracer. It works by
compiling Haskell source files to annotated -- but still textual -- Haskell
source files. After this source-to-source translation, the user compiles the
annotated source code and runs it to produce a Hat trace.

The trace is a rich recording which contains high-level information about each
reduction the program performed. Hat comes with a number of utilities for
exploring the trace files, including some forms of forward and backward
debugging, filtering utilities which show all arguments passed to top-level
functions, virtual stack traces, and even an interactive tool for locating
errors in a program, similar to one of the features of Hoed.

\todo{Rewrite comment into text, scratch the paragraphs below}
% TODO so initially it was developed for the nhc compiler, gaining Haskell 98
% features and GHC support later on. The history of haskell paper
% https://dl.acm.org/doi/pdf/10.1145/1238844.1238856 has more info on this in
% section 10.4.2. Even in its prime time (?) it wasn't recognised as a useful
% tool, despite its many features.

The architectural decisions of Hat reflect the environment it originated in,
which unfortunately differs substantially from the current status quo. Its
source-to-source model of operation makes it compatible with various Haskell
compilers,


The Glasgow Haskell Compiler is the most widely used Haskell compiler
\citationNeeded with many language extensions beyond Haskell 2010. In 2009,
\acrshort{ghc} became the official compiler of the Haskell
Platform\cite{haskell-platform}, further cementing its monopoly as the primary
implementation of the language.

Hat uses the \hackage{haskell-src-exts} package to parse the source language.

% TODO \texttt{haskell-src-exts} DOES NOT lack feature parity with GHC
% TODO oblivious to laziness, implementation-agnostic

\subsection*{\hackage{htrace}} \label{sec:htrace}
\hackage{htrace} \citationNeeded is a simple package which exports a single
function: \hsSignature{htrace :: String -> a -> a}. As the name and function
signature suggest, this function mirrors the behaviour of the standard
\hsIdent{trace}, except that when displaying the tracing messages,
\hackage{htrace} shows them hierarchically indented based on the current call
depth. It works simply by manipulating a global mutable variable and hiding
this fact from the user with \hsIdent{unsafePerformIO}.

Although very simple and oblivious to any laziness implementation details, this
approach is still useful for debugging purposes. The indented tracing messages
suggest the depth to which various thunks are evaluated at different points of
the program's operation.

\subsection*{\hackage{ghc-heap-view}} \label{sec:ghc-heap-view}
\hackage{ghc-heap-view} is a Haskell package which makes advanced introspection
of the Haskell heap a possibility from within pure Haskell code. It relies on
the \hackage{ghc-heap} library which comes bundled with \acrshort{ghc}.

The library's notable high-level features include a function which attempts to
recreate readable Haskell source code from a runtime value, using \hsCode{let}
bindings to express sharing. There are also tree and graph data structures for
heap mapping and a high-level algebraic data type for all Haskell closures,
complete with their info tables.

\todo{To-do}:
\begin{itemize}
	\item explain trade-offs with those that need code changes (typeclass-based)
	\item explain problems with approaches independent of \acrshort{ghc}
\end{itemize}

Despite Haskell users' considerable interest in avoiding the implicit delaying
of computations which the language is notorious for, there are no records of a
large-scale study of the use of laziness in practice akin to
\cite{emp-study-laziness-r}. The tool with a feature set closest to what is
necessary for a comprehensive analysis of the practical use of laziness is
likely \hackage{ghc-heap-view}, which allows the user to interactively inspect
the heap objects and look inside thunks using \acrshort{ghci}. However, the
package primarily provides a rich library interface. It does not implement a
tracing mode, which would facilitate collection of laziness-relevant
information during the execution of entire programs.

\subsection{Summary} \label{sec:summary}
Table \ref{tbl:thunk-manager-comparison} summarizes the surveyed tooling.

% \begin{savenotes} % support for footnotes within a tabular environment
\begin{table}[h]
	\centering
	\begin{tabularx}{\textwidth}{|| l *{3}{L} c ||}
		\hline
		Tool
		& Source changes
		& Order of evaluation
		& Thunks
		& Memory awareness\footnote{The surveyed programs span several layers
			of abstraction, with tools such as \nameref{sec:hoed} being completely
			oblivious to implementation details, \nameref{sec:nothunks}
			dependent on the runtime representation of values but not directly
			exposing it to the user, and \nameref{sec:ghc-heap-view} reifying
			implementation details.}
		\\ \hline \hline

		\nameref{sec:hoed}
			& Required    % source changes
			& Recorded    % order of evaluation
			& Transparent % thunks
			& None        % memory awareness
			\\ \hline
		\nameref{sec:nothunks}
			& Required    % source changes
			& Ignored     % order of evaluation
			& Detected    % thunks
			& Limited     % memory awareness
			\\ \hline
		\nameref{sec:hat}
			& Unnecessary % source changes
			& Recorded    % order of evaluation
			& Transparent % thunks
			& None        % memory awareness
			\\ \hline
		\nameref{sec:htrace}
			& Required    % source changes
			& Illustrated % order of evaluation
			& Transparent % thunks
			& None        % memory awareness
			\\ \hline
		\nameref{sec:ghc-heap-view}
			& Unnecessary\footnote{\nameref{sec:ghc-heap-view} does not require
				any changes to code which allocates the closures it is able to
				inspect.}
				% source changes
			& Ignored     % order of evaluation
			& Reified     % thunks
			& Full        % memory awareness
			\\ \hline
	\end{tabularx}
	\caption{An overview of existing solutions to thunk discovery and laziness
	debugging.}
	\label{tbl:thunk-manager-comparison}
\end{table}
% \end{savenotes}

\section{Existing profilers} \label{sec:existing-profilers}
\todo{...}

\subsection{Haskell Program Coverage} \label{sec:hpc}
Haskell Program Coverage\cite{hpc-paper} is (unsurprisingly) a code coverage
tool for Haskell. Similarly to Hat, \acrshort{hpc} has a source-to-source mode
of operation but additionally offers tight integration with \acrshort{ghc} and
comes bundled with modern releases of the compiler. It supports all
\acrshort{ghc} language extensions.

\acrshort{hpc} allows easy instrumentation of arbitrarily complex Haskell
programs without source annotations. It wraps subexpressions in the program
with an unsafe side-effecting function which records its evaluation by mutating
a module-wide array of integer counters. The final state of the per-module
arrays forms the \acrshort{hpc} trace. This architecture is wired into the
\acrshort{ghc} compiler pipeline in all the major data structures (the surface
syntax, Core language, and \acrshort{stg}), which makes it both robust and
per\-for\-mant. The tool comes bundled with utilities for displaying the
original source code with colourful mark-up, highlighting interesting
subexpressions based on the information extracted from the trace. Notably,
\acrshort{hpc} supports traces of the boolean values of pattern guards, which
are added to the visualisation.

\acrshort{hpc}'s feature set can be of tremendous help to the Haskell
programmer, especially when combined with tools like
QuickCheck\cite{quickcheck-paper}. However, its traces are tuned specifically
for code coverage and do not contain enough information to be useful for any
kind of dynamic strictness analysis. While the \acrshort{hpc} traces are
sufficiently granular, the subexpression counters lack necessary information
about their execution context and timing.


\section{The Glasgow Haskell Compiler} \label{sec:ghc}
\todo{Explain that several previous approaches had a direct influence on the
compiler. \acrshort{ghc} now has \acrshort{hpc}-specific features, compiler plugins, which
supersede source-to-source transformations (strengthening the monopoly but
simplifying implementation and streamlining the process), and a codebase that's
increasingly amenable to various extensions via the trees that grow pattern,
\hsIdent{Tickish}, etc.}
\blind[1]

\subsection{Compiler plugins} \label{sec:compiler-plugins}
\blind[1]


\chapter{Analysis and design} \label{sec:analysis-design}
\todo{what is analysis?}
\blind[1]

\section{Analysis} \label{sec:analysis}
\begin{itemize}
	\item the core question: is laziness worth the hassle?
	\item original approach by modifying \acrshort{ghci}
	\item reuse the implementation of breakpoints, effectively implementing
		fairly clean hooks (bearing a slight resemblance to how the R tracer
		works)
	\item build on \texttt{Tickish}
\end{itemize}

\subsection{The problem} \label{sec:the-problem}
\todo{really? ``The problem''?}

The non-strict semantics of the Haskell language were a guiding principle which
influenced or directly determined many of the decisions made at its
inception\cite{history-of-haskell}. However, the implementation of non-strict
features via laziness in \acrshort{ghc} brings many pitfalls which Haskell
programmers need to deal with. Automatic avoidance of unnecessary thunk
allocations is conservative\citationNeeded: if \acrshort{ghc} is unable to
prove the strictness of a function in an argument by static strictness
analysis, the function will remain lazy, often leading to pathological memory
behaviour at runtime.

A simple and popular method of dealing with undesired laziness is the language
extension \texttt{BangPatterns}\citationNeeded [cit. both for the popularity
and for the extension itself]. \texttt{BangPatterns} introduce a new syntax for
forcing an expression to \acrshort{whnf} when pattern-matching on
it\todo{clarification needed}.

\todo{the following paragraph is a plan on what to write} explain that bang
patterns are often only added after something goes wrong and the compiler
doesn't help with choosing where to place them (or indeed with any memory
``leaks'' at all). Haskell keeps making things lazy and the developer keeps
running into various bugs due to laziness, in a kind of whack-a-mole game
(although that isn't exactly right, fixing one memory leak doesn't introduce
another). Note also that Idris, a language inspired by Haskell, chose purity
but not laziness.

This fight against the semantics is detrimental to the developer experience of
the language. The question arises whether the benefits of laziness outweigh the
toll it takes on the programmer. To answer this question, the runtime behaviour
of lazy features needs to be understood. As a first step towards that
understanding, we design a dynamic tracing tool capable of capturing
information about the runtime behaviour of non-strict functions.

\subsection{Approach} \label{sec:approach}
\begin{itemize}
	\item core question: how is laziness used in practice?
	\item to understand that, we have to find out whether functions are strict and
		to what extent, discover potential strictness dependencies between their
		arguments, etc
	\item to do that, we need to determine whether an argument has been
		evaluated during function application
	\item to do \textit{that}, we have to look at runtime values
	\item to put the observations of runtime representations into context, we
		have to somehow keep track of function calls
\end{itemize}

The goal of this work is to design and implement a tool suitable for
understanding \todo{iffy} how is laziness utilised in real-life Haskell
programs. To analyse the practical implications of \acrshort{ghc}'s
implementation of non-strictness, we have to understand the strictness
properties of functions. For example, some arguments may be evaluated if and
only if others are. The tool must capture these dependencies and usage
patterns, as they may uncover both use cases where laziness is essential and
places where it could be safely avoided, even though static analysis cannot
determine so.

Dynamically inferring the strictness properties of functions requires a peek
under the hood of Haskell's runtime machinery. Typical Haskell code is
oblivious to the underlying representation of the values it manipulates, as
reification of thunks would weaken equational reasoning and parametricity.

\todo{nope, this is a false dichotomy}
There are two general approaches one could take to capture the information
about runtime structures over the execution of a Haskell program: modify the
program, or modify the compiler. The former would involve rewriting the source
code, while the latter

The purpose of the
project thus dictates use of features which violate some of the abstractions
provided by the language\todo{does it really? We're interested in GHCi as
well...}.

Understanding the use of laziness at runtime requires insight about runtime
structures that are otherwise transparent to the Haskell programmer. A key
feature of the language is its support for equational reasoning, which would be
broken if thunks were directly observable. To determine whether certain values
have been evaluated or not, we need to observe state that is typically hidden
from a Haskell program.

Once we have the power to inspect the runtime representations of values, we
need to use it to determine the strictness of functions. A function \hsCode{f}
is strict in an argument \hsCode{a} if \hsCode{a} has to be evaluated whenever
\hsCode{f a} is evaluated.

\subsection{The \acrshort{ghci} approach} \label{sec:ghci-approach}
Taking inspiration from \cite{emp-study-laziness-r}, the original
implementation plan was to work with \acrfull{ghci}. The bytecode compiler and
interpreter lack support for certain \acrshort{ghc} language extensions, namely
un\-box\-ed tuples and sums, but the supported subset of the language was
considered large enough to contain interesting examples\todo{maybe
``specimens''?}. The relative simplicity of the bytecode compilation pipeline
and the fairly straightforward evaluator were considered to provide a
foundation amenable to low-level tweaks deemed necessary for the extraction of
crucial tracing information.

The framework of the interpreter would ease the implementation of certain
features. \acrshort{ghci} already implements breakpoints, which pause the
execution of a Haskell program running in a separate thread and pass messages
to the controlling Haskell thread.

\subsubsection{\acrshort{ghci}: a primer}
\acrshort{ghci} is an interactive interface built on \acrshort{ghc}'s bytecode
compilation pipeline and the bytecode interpreter of the \acrshort{rts}.
\acrshort{ghci} offers a read-eval-print loop popular in other programming
languages.

\acrshort{ghci} consists of several key components: the \acrshort{ghci} UI, the
\acrshort{ghci} debugger, the bytecode generator, and the bytecode interpreter.
The former two are a part of the front end of \acrshort{ghc}, while the
bytecode-centric parts fit into the back end of the compiler pipeline and the
\acrshort{rts}, respectively.

The following sections will introduce each of the building blocks from which
\acrshort{ghci} is composed, starting with an overview of how they fit
together.

\subsubsection{The life of an interpreted expression}
The user's expression entered at the \acrshort{repl}'s prompt is fed through a
modified \acrshort{ghc} pipeline, as \acrshort{ghci} expects Haskell
expressions, not top-level definitions. This modified pipeline culminates in
bytecode generation, producing a collection of bytecode objects together with
high-level information about breakpoints, pointers to allocated string
literals, and other data.

The bytecode objects form, together with other information, a compiled
module\todo{is this correct? don't modules exist right after renaming?}.
That module is loaded by the compiler instance and \todo{uhhh...}

When evaluating an expression, the UI forks a new thread to perform evaluation
independently of the interface. This ensures that exceptions raised during
evaluation of an expression don't crash \acrshort{ghci}. The UI forwards
exception handlers appropriately to ensure this is the case. The two threads
communicate via \textit{mutable variables}, or \hsType{MVar}s. These are
concurrency primitives from the \hsModule{Control.Concurrent.MVar} module which
effectively implement concurrent, mutable
\hsType{Maybe}s\cite{concurrent-haskell}. A mutable variable of type
\hsType{MVar a} contains either no values or a single value of type \hsType{a}.
It can be safely shared across threads and supports operations
\hsIdent{takeMVar} and \hsIdent{putMVar}. The former operation extracts the
value stored in an \hsType{MVar}, leaving the variable empty if a value is
present. If the variable is empty, the operation blocks. The converse\todo{is
this the right word?} operation \hsIdent{putMVar} blocks on a full variable and
fills it with a value as soon as it is empty.

Two \hsType{MVar}s play an important role in the design of \acrshort{ghci},
\hsIdent{statusMVar} and \hsIdent{breakMVar}. These variables form a
communication channel between the UI thread and the thread responsible for the
evaluation of an expression.

\todo{either reuse for the UI section or get rid of this}
They are greeted with the interpreter's UI\todo{is it necessary
to include this in the acronyms?} and can begin writing Haskell expressions
directly or first invoking various \acrshort{ghci} commands to load modules,
print types, kinds, and documentation, browse the contents of modules and
perform other tasks.

\subsubsection{Bytecode generation}
The bytecode facilities of \acrshort{ghc} involve a detour from the typical
sequence of steps performed to transform Haskell sources all the way to a form
suitable for linking or execution. After desugaring, the program is transformed
directly into bytecode instructions\footnote{Note that this approach will soon
be replaced by a new bytecode pipeline which follows the usual compilation
process all the way to \acrshort{stg}\citationNeeded.}. Optimisations
implemented in the simplifier are not performed. \acrshort{ghci} is intended
for interactive evaluation and favours fast, iterative development over runtime
performance, making the naive code generation approach a reasonable choice.

Every top-level definition, every scrutinee of a \hsCode{case} expression, and
every right-hand side of a non-trivial \hsCode{let} expression are compiled to
a \acrfull{bco}. Such an object contains an array of bytecode instructions
together with \todo{finish this}

The bytecode format comprises 67 instructions
\todo{describe the instructions in a table}
\todo{stack/heap checks prevent (uh, or maybe react to?) respective overflows}

\subsubsection{The bytecode interpreter}
The interpreter which \acrshort{ghci} relies on is a part of the
\acrshort{rts}. Its primary workhorse is the \texttt{interpretBCO} function
which handles closure evaluation, unboxed returns, function application, and
interpretation of bytecode instructions. For tasks it is unable to deal with,
such as application of machine-code functions, it returns to the scheduler.

Interpretation works simply by case analysis on the current instruction.

\subsubsection{The debugger}
A notable feature of the tool is the \acrshort{ghci} debugger, which allows the
programmer to place breakpoints on certain expressions in their code. The
interpreter then pauses execution when it is about to evaluate an expression
marked by a breakpoint.

Due to laziness, the order in which breakpoints are hit depends on the order in
which their respective thunks are forced to \acrshort{whnf}, not directly on
the order in which functions are called. Breakpoints thus equip the Haskell
programmer with a powerful tool for debugging order of evaluation issues caused
by the language's non-strict semantics.

Internally, breakpoints rely on a special bytecode instruction called
\texttt{BRK\_FUN}. Upon encountering this instruction, the interpreter first
checks whether it is already returning from a breakpoint (via a flag in the
\acrshort{tso}). If it is not returning from a breakpoint and the associated
breakpoint is enabled, the interpreter pauses execution at this point.

Pausing on a breakpoint is quite an involved action. The interpreter prepares
to call an ``IO action,'' which is a Haskell function invoked to resume
\acrshort{ghci}'s UI thread by filling the shared mutable variable. This
preparation saves the top stack frame to a new closure, a pointer to which is
passed to the IO action. The stack is then set up to call the IO action, and
the interpreter returns to the scheduler in order to perform the call.

At no point is the instruction pointer persisted -- the progress of evaluation
of the current \acrshort{bco} is lost whenever the interpreter stops at a
breakpoint. This is acceptable, as the bytecode generator makes sure to only
put \texttt{BRK\_FUN} instructions at the very start of bytecode objects and
the \acrshort{tso} flag ensures that a just-visited breakpoint is not stopped
at again.

\subsubsection{The user interface}
\ldots

\subsection{The compiler plugin approach}
To produce useful tracing output, a dynamic tracing framework must capture
interesting events during a program's evaluation and relate them to one
another. In particular, the evaluation of function arguments must be clearly
related to the respective function call to enable reasoning about the
strictness of a function on a call-by-call basis. While retaining the order of
evaluation is trivial in a call-by-value language, laziness introduces
interleaving. This can only be dealt with by the introduction of state into the
program (or into the tracing framework) in order to recover the dependencies
between function calls and argument evaluations, which are no longer implicit
in the order of the trace events.

It is this function-call-specific state that becomes difficult to express
without high-level information about the program structure at hand, as was the
case with \nameref{sec:ghci-approach}\todo{lowercase}.

\subsubsection*{Rewriting argument references}
\todo{A manual introduction of state into the program is rather trivial\ldots}

Fortunately, function-call-specific state can be easily introduced into the
source program, simply as local variables. It suffices to keep a unique
identifier of the particular function call that the argument evaluation traces
can refer to. Such a unique identifier necessarily needs to change with every
function call. In clean Haskell code without unsafe features, this is
impossible in general, as the language requires the use of the \hsType{IO}
monad in order to perform side-effecting computations.

Since rewriting functions into a monadic form would be a difficult undertaking,
we prefer the way of unsafe features. Integer counters are enough for call
identification purposes, so we choose to keep one counter per function. All
counters can be stored in a single mutable map, which associates

to-dos:
\begin{itemize}
	\item describe the general idea of the source plugin
	\item explain the use of SYB
	\item explain the use of TH and splicing
	\item explain how the global mutable map resides in the plugin's module
	\item explain the difficulty of trying to implement similar functionality
		in a core plugin instead
\end{itemize}

Equipped with a means of introducing benign side-effects into programs for
tracing purposes, we are in search of a way of rewriting source code to put
these side-effects to use. One plausible approach would be direct source code
rewriting, akin to \nameref{sec:hat}. As described in section
\ref{sec:existing-tools}, source-to-source transformations have the benefit of
generality, but also the downside of additional complexity in both the
rewriting process itself and the build process of the program, which the user
of our tool would have to deal with. Furthermore, true implementation
agnosticism of the tracing framework would require compiler-independent support
for inspection of the Haskell heap, for which no solution seems to exist at the
time of writing\todo{right?}. A less general but more ergonomic way of
rewriting source code is via \acrshort{ghc}'s \textit{source plugins}, which
hook directly into the compiler pipeline and can operate on the surface-level
syntax at different stages.

\subsubsection*{Source plugins}
Source plugins\cite{ghc-source-plugins} are a relatively recently introduced
feature of \acrshort{ghc}. Compiler source plugins are Haskell packages which
invoke the \acrshort{ghc} API\todo{same as UI, I suppose?} to hook into the
compiler pipeline and modify the compiled program at various stages of the
front-end. Unlike Core plugins\citationNeeded, which operate on the internal
language, source plugins deal with the entirety of Haskell's surface syntax.

Rather than parsing, transforming, and serialising the source code separately
to the compilation step, we can design a plugin that performs the required
source transformations in the compiler pipeline directly. We introduce two
tracing functions, \hsIdent{traceEntry} and \hsIdent{traceArg}, into the
current module. We then rewrite the source program to call \hsIdent{traceEntry}
every time a function in the program is invoked and we thread every reference
to a function's argument through \hsIdent{traceArg}. This introduces the
opportunity to inspect the runtime representations of the arguments passed to a
function when the result of the function is under scrutiny.

We can determine the strictness properties of a transformed function from the
calls it makes to the tracing utilities. If we record a call to a (transformed)
function \hsCode{f :: Int -> Int -> Int} defined as \hsCode{f x y = \ldots} via
\hsIdent{traceEntry} but no calls to \hsIdent{traceArg}, the function makes no
use of its arguments.

\subsubsection*{Rewriting the \acrshort{ast}}


\chapter{Realisation}
\ldots

\section{Development environment}
\ldots

\section{Building \acrshort{ghc}}

The \acrshort{ghc} codebase is a large and complicated collection of source
files written in a number of programming languages, primarily Haskell and
C\cite{arch-ghc}. The ever-evolving project is supported by a custom build
system called Hadrian\citationNeeded, itself written in Haskell, which
bootstraps the self-hosting compiler in several steps. To build \acrshort{ghc},
an appropriate version of \acrshort{ghc} has to be installed already. The
installed compiler is referred to as the \textbf{stage 0} compiler \todo{fix
the formatting of stages}. Hadrian uses the \textbf{stage 0} compiler to build
first the Hadrian build system and with it the \textbf{stage 1} compiler, which
is a freshly built \acrshort{ghc} linked against the \textbf{stage 0}
\hackage{base} library. The \textbf{stage 1} compiler is subsequently used to
build the core libraries from scratch. It is then utilised again to build the
\textbf{stage 2} compiler, which is linked against the freshly built
\hackage{base}. The \textbf{stage 2} compiler constitutes a complete build of
\acrshort{ghc} from source code. There is an optional follow-up step, where the
\textbf{stage 2} compiler builds a \textbf{stage 3} compiler, which is useful
for profiling \acrshort{ghc} while building \acrshort{ghc}.

The first step to working on the project after obtaining the source code is
setting up the build system. Since specific releases of \acrshort{ghc} require
specific \textbf{stage 0} compilers as the project quickly adapts to use new
language extensions, the management of \acrshort{ghc} versions on a Unix-like
system with a system-wide package manager can be difficult. To ease the
management of installed versions and enable quick switching between them, the
\texttt{ghcup} tool\cite{ghcup} has been developed. \todo{this will need some
more citations} \texttt{ghcup} lets the \acrshort{ghc} developer quickly
install and switch between the releases of not only \acrshort{ghc} itself, but
also Cabal, the Haskell build system and dependency manager, and the
\acrfull{hls}, an \acrshort{lsp}-compliant language server providing
Haskell-specific editor integration features.

There are several supported approaches to building \acrshort{ghc}, as the
compiler previously used a build system based on \acrshort{gnu} Make (before
switching to Hadrian) and the old Make build system is still being phased out.
Additionally, the build tool of the programmer's choice can be combined with a
Docker or Nix -assisted set-up, simplifying the installation of other
dependencies required for the build process.

\todo{how do we say ``let's not pick Make tho?''}
\todo{ways and flavours!}
After the initial build, the \textbf{stage 1} compiler can be \textit{frozen}
by passing a flag to the build system on subsequent invocations. This prevents
rebuilding the \textbf{stage 1} compiler every time a source file changes, which
speeds up the edit-compile-run cycle tremendously.

\blind[3]




\setsecnumdepth{part}
\chapter{Conclusion}

\blind[2]

\bibliography{bbl}
% TODO get ISO 690 working
\bibliographystyle{plain}

\setsecnumdepth{all}
\appendix

\chapter{Acronyms}

\todo{fix acronym glossary}
\printglossary[type=\acronymtype]


\chapter{Contents of enclosed CD}

\todo{figure out what to do about this}
%change appropriately

\begin{figure}
	\dirtree{%
		.1 readme.txt\DTcomment{the file with CD contents description}.
		.1 exe\DTcomment{the directory with executables}.
		.1 src\DTcomment{the directory of source codes}.
		.2 wbdcm\DTcomment{implementation sources}.
		.2 thesis\DTcomment{the directory of \LaTeX{} source codes of the thesis}.
		.1 text\DTcomment{the thesis text directory}.
		.2 thesis.pdf\DTcomment{the thesis text in PDF format}.
		.2 thesis.ps\DTcomment{the thesis text in PS format}.
	}
\end{figure}

\end{document}
